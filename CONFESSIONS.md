# My First Foray into AI-assisted coding

NOTE: This isn't a summary. More like a play-by-play. But the actual logs
were to the tune of around 30MB+. I ain't posting that.

It's (almost) full disclosure time. The GUI encoder was, as stated in README.md,
programmed using AI assistance. The specific tool used is a VSCode plugin 
called `Cline`, using the Gemini 2.0 Flash and Gemini 2.5 Flash models provided
by Google Gemini. The API key that I used links to a paid account that belongs
to a friend who I won't name (unless I'm told to) but someone whom you may know
if you've been around the Cemetech channels long enough. Here's a breakdown
of (some) of the work that's been done on-and-off during the past
(almost) three months.

A fair bit of the active time was spent was figuring out what the heck I could
even do with the AI. My friend told me to treat the thing as an overenthusiastic
intern with attention problems but a wide range of knowledge. Just ask it stuff.
Cline keeps a log of everything I ask of it and the replies it gives along with
what it does. Posting all of that would take up tens of megabytes worth of
text that I'd rather not have cluttering up my repository, so I'll just give
you all the highlights.

## Things I've learned overall

* Be exceedingly clear with what you want done.
* If you refer to a piece of a project or structure with a name, be sure to
  stick with that name, and to never use terms loosely. Stuff goes awry if you do.
* When creating a GUI, its spatial awareness is absolute garbage. You'll spend
  most of your time rearranging things, but it beats keeping twenty tabs worth
  of TKinter documentation open at all times. Now you only need like five!
* Variable naming is important. The AI can actually infer what elements of the
  UI you're talking about when you want to make changes. (e.g. If you make a
  group called "trim_toolbar_group" then ask it to do something like "Add a
  button to the trim toolbar", it'll get that right.) Though to be honest,
  those sort of names will be autogenerated if you originally asked to "Add a
  toolbar. Its purpose will be to let the user trim the video." or somesuch.
* Keep clear documentation regarding your project. If you're dealing with a
  file format, ensure that all terms you use have meaning. Be as precise as
  possible, and when you get your AI to use it when creating routines involving
  that, it's less likely to screw up (and pretend it's all fine).
* Make sure that documentation is actually correct. Spending time debugging
  things because you misunderstood which way bitfields are aligned sucks.
* Actually READ the stuff it spews out. ESPECIALLY if you ask it to do things
  you don't actually know how to do. You'll not only broaden your understanding
  of such things (i.e. how to add drag and drop support to a TKinter window),
  but be apprised of any budding screwups that might happen.
* When the AI starts obviously hallucinating (writing clearly wrong things),
  it's time to click the "Restore" button on the last prompt where it did
  something useful, then start a new task.

## The Play-by-Play. It's mostly in order.

NOTE: This section is disjointed as hell. It's because I haven't figured out
how to export ALL tasks in Cline to file. And each time I try to click a task,
going back dumps me back at the start of the list of tasks. And the list of
tasks is LONG.

With that out of the way, let's get to it. My first few prompts were basically
boilerplate material and trying to get Cline setup with the project. Stuff that
was in the documentation for the tool. Stuff that I botched a couple of times.

After that, I told it to "Update this project to use pipenv." Since this is a
Python project, I wanted to make sure my dependencies didn't go anywhere. Fat
lot of good that did, since I didn't actually export that functionality to
the main branch. I then asked it to read the contents of the project and to
create documentation for it. It went... poorly. I abandoned further attempts.

I then asked the AI to add docstrings to all the functions in all the files of
my project. That... did not go as well as I thought. It did do the thing, but
most of the documentation was inane. The few that weren't were surpisingly
insightful. Glad I didn't have to write all that!

I was working with a prior version of the UI (it barely worked) when I asked it
to go ahead and create a new file that'll contain a class I call "MediaFile".
In it, I told the AI step by step what I wanted out of it. I put in six
different tasks as it kept screwing things up. The first time around was
almost right. Basically, I asked it to do some basic input validation and then
do extended validation using `ffprobe` and capturing its output. Then it kept
doing increasingly convoluted things when I asked it to keep adding to the
validation as I kept coming up with new requirements for the thing. At some
point, I just nuked the file and started from scratch. Funny thing. The AI
somehow *remembered* what it was I was ultimately trying to get out of both
validation and file decoding. At the very end of it, I got the AI to output
PIL-compatible frame data for the media file I input, fixed to 30fps and fixed
to the maximum resolution I'll be displaying on the calculator, which is a base
of 288 by X, where X is scaled. Included in the prompts were requests to make
a test that performed the verification of stuff.

**THAT DAY I LEARNED**
1. Know the requirements in full before you try. If you don't, be prepared to
   nuke all your progress and try again.
2. Using `ffmpeg` to try to output APNG doesn't work if you attempt to pipe its
   output to `stdout`. Apparently, creating an APNG file requires that the
   stream supports rewinding (which `stdout` does not), and my version of 
   `ffmpeg` *doesn't tell you about this, nor makes any noises regarding it*.
   A whole day down the drain.
3. `pytest` is something I never knew I needed. You'll also be burning a bunch
   of credits as the AI debugs the code it wrote using the results of those
   tests.

Then I spent a few more prompts getting the AI to craft a new UI for this
project. Most of the prompts consist of stuff like "create this button" or
"move this widget there". I quickly learned that the AI is *absolute trash at
spatial awareness*. Sometimes it gets things right. But most of the time, I
spent manually moving objects or changing the geometry manager, and THEN moving
stuff around. In the end, though, I got a roughly serviceable UI that can call
a placeholder function intended to perform the actual encode process.

I then added a data processing module and asked the AI to create the outline
of that. I ended up making the `Cevideomode` class myself since I wouldn't have
trusted the AI further than I could throw it at this point. Making sure that
video modes were kept firmly under my thumb was important. Anyhoo. I asked it
to build the outline of the `Cevideoframe` class, which contains all the needed
extra data for each frame, including its display-read form, and its data
encoding.

Ok, I lied about `Cevideomode`. I used the AI to add equality testing since I
couldn't be arsed to write the stupidly simple `__eq__` magic method. I still
had to manually modify it later on.

I then made more fixes to the UI. An example of stuff I couldn't be arsed to
read the documentation for included such prompts as "We'll be making a few
changes to cev_ui.py. First, make a change that would prevent the user from
resizing the window." A much longer prompt is as follows, and it really does
deserve its own paragraph:

"We'll need to collect additional information about the video  that is to be exported. A name for the video to export must be supplied. Do not allow the Export Video button to become enabled until after a valid name is input. A valid name is a string of no more than 8 alphanumeric characters and no less than 1. Those alphanumeric characters may not contain lowercase letters. The first character of the string may not be numeric. Present these rules to the user in a tooltip when they hover over the input box. The default name will be "UNTITLED". A video title may be supplied. It can contain any alphanumeric characters. It is assumed to be at most 40 characters wide, but the actual upper limit is 255 characters. A video author may be supplied. It shares the same limits as the video title. The UI elements for these three boxes will be placed between the row where the scrollbar is and the row where the Import Media and Export Video buttons are."

In that same task (and the next few tasks), it screwed up the tooltip. Badly.
It also screwed up input validation in a way that I would have felt comfortable.
At some point, I tried to implement this myself, but I gave up since TKinter's
idea of on-the-spot input validation involved a complex mode string linked to
a bunch of routines that does far too little to actually do a damned thing, and
the only binding that would have solved the problem couldn't actually solve
anything since *solving the problem isn't what it was meant for*. I was 300%
DONE with it at that point. I just had the AI put in an interim fix that
disabled the "Export" button until the name that's been put in was correctly
formatted.

Nearby, I had it perform a task which I still regard as the best thing the AI
did for me for this entire project. Which was add a progress bar for importing.
This was about as close to "vibe coding" I ever got, and that was still with
me restarting the task or two. It all started with the prompt
"I want to add a modal dialog box that appears over the center of the canvas when a video is imported. The message in the box will read "Importing video...". The dialog box will go away once the video import is completed" and a few more prompts that fixed
box placement (it initially tried to center the modal to the screen, not app),
it finally did it. 

**I also learned about the magic of the `threading` module**

Now. It was at this point I needed to solidify my understanding of just what
the export feature is going to output. To that end, documentation was needed.
The following prompt started it. There was much manual editing. SO MUCH.

"Hey, AI bro. I've tried to rewrite some documentation in FILE_FORMATS.txt but I never finished. Help me make it more concise and understandable, and write the results in a new markdown file. The stuff in that file is going to be used later on in writing a bunch of export routines that'll make use of it"

After that, many more tasks were created to try to flesh out the functionality
of the UI, including additional details of what the "Play" button is supposed
to do, along with creating a new toolbar that hides behind the existing one
to contain more advanced options. As of the time of this writing, nothing was
really done with it. I plan to. I swear!

I then asked it to rewrite (read: edit for clarity) and test the function used 
to export data to .8xv files. That... did NOT go well. Mostly because I was an 
incompetent idiot who went about testing it all kinds of wrong. Burned maybe a
few days on that fruitless endeavor.

At some point, I asked the AI to create a `Cevideolist` object to create from
a `MediaFile` and a `Cevideomode` object a list of `Cevideoframe` objects. But
at this point, I fleshed out its functionality. Given the file format
documentation, I had it create methods to group video frames, encode them, and
then compress chunks of them. I then asked it to make a method to sort through
the chunks, collect them, add metadata, and then arrange them in blocks that
would eventually make their way to .8xv data files in a manner that maximizes
how much would fit in each.

I then made several prompts to fill out the `encodeframe` method. The AI is
smart enough to know where it is so I provided no other details aside from
"in cevencode.py" (as it was named at the time). It... did it surprisingly well.
Right up until the point where the small details started hitting me in the face,
such as window sizing, and byte alignedness that I utterly failed to document
at that point. I tried a few more prompts to get that to fix, but I ultimately
had to manually adjust bounding box offsets for partial frame types. I tested it
by having omit any grid frames (which I also knew wasn't working), then sending
the results to the emulator for further testing.

At some point in the middle of the paragraph above, I got tired of trying to
manually convert .png files output by Cemu into .gif files that I would share
with the fun people on Discord, so I took the super lazy way out.

"In png2gif.bat, write a batch script to convert a video file to a .gif file
using ffmpeg. The input file will be provided via drag and drop"

This file lives on my desktop. I had to manually modify it because I
didn't like how it was scaling up things.

**I then stopped working on this project for a few weeks. How do I know? Because
all the prompts that happened after this were for a different project**.

When I got back to it, I tried to get it to figure out what the heck was going
wrong with the grid packing part of `encodeframe`. At this point, I had
switched over to Gemini 2.5 Flash. Turns out 2.5 is much, much smarter than 2.0.
Which helped quite a bit, but not as much as I needed it to. I learned a few
things in that process:

* The AI was having problems seeing through the original code's wonky method of
  constructing the bitfield needed to parse the grid array.
* The AI was absolutely confident in its wrongness.
* The AI is surpisingly adept at parsing ez80 ASM code. Like. wtf. I somehow
  managed to keep it on track long enough to see through all the SMC bullshit
  that my decoder threw in.
* YOU ABSOLUTELY MUST READ ITS THOUGHTS. It was how I rediscovered all the
  stupid little quirks that my decoder has. And all the crap it pulled.
* The AI will absolutely gaslight you if you aren't 100% firm with what you're
  trying to tell it. It helps to tell it where the ultimate source of truth is
  in the project. In my case, it was the ez80 ASM code.
* When you're done overcoming all the crap it threw at you, tell it what you
  did to fix it your goddamned self, and to update the documentation with
  those discoveries. It's at least competent enough to do that.

So. Yeah. After that got fixed and verified, I went in to do some manual editing
of my readme file, gave it one more prompt to treat the UI files as a package,
fixed the unit tests that I definitely did not include, and wrote this steaming
pile of garbage of a file. I refuse to use the AI to clean up this monument to sin.

## There you have it.

A way to humor my friend. A way to get started on a project that I was floundering
at. And an experiment to see how far I could take it. The encoder at the time of
this writing still lacks an adaptive encoder. Don't care. I just wanted something
to ship.

